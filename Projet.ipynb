{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet réseaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 0 : Charger les données\n",
    "file_path = \"data/12.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "tuples = [(l.split('`')[0], l.split('`')[1].strip()) for l in lines]\n",
    "\n",
    "# Vérification de l'existence des fichiers\n",
    "result = [(f\"data/{item[0]}\", item[1], os.path.exists(f\"data/{item[0]}\")) for item in tuples]\n",
    "\n",
    "data = []\n",
    "for file, label, exists in result:\n",
    "    if exists:\n",
    "        data.append((file, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = data[0:10000]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Découpage du mot en caractères"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonnes correspondances = 2974\n",
      "Mauvaises correspondances = 7026\n",
      "Caractères collés = 3330\n",
      "Caractères coupés = 3696\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def split_text_into_characters(image_path):\n",
    "    \"\"\"\n",
    "    Découpe une image de mot en images de caractères individuels.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): Chemin vers l'image du mot.\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des images des caractères en format PIL.\n",
    "    \"\"\"\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Prétraitement : Binarisation adaptative\n",
    "    thresh = cv2.adaptiveThreshold(\n",
    "        image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2\n",
    "    )\n",
    "\n",
    "    # Suppression du bruit (ouverture morphologique)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "    denoised = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    # Détection des contours\n",
    "    contours, _ = cv2.findContours(denoised, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filtrer les petits contours (artefacts)\n",
    "    contours = [c for c in contours if cv2.contourArea(c) > 20]\n",
    "\n",
    "    # Trier les contours de gauche à droite\n",
    "    contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[0])\n",
    "\n",
    "    characters = []\n",
    "\n",
    "    # Découpage des caractères\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        char_image = image[y:y + h, x:x + w]  # Découper l'image du caractère\n",
    "        char_pil = Image.fromarray(char_image)  # Convertir en format PIL\n",
    "        characters.append(char_pil)\n",
    "\n",
    "    return characters\n",
    "\n",
    "\n",
    "def process_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Traite un jeu de données pour découper les images de mots en images de caractères.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (list): Liste de tuples (chemin_image, mot).\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste de tuples ([images des caractères], [caractères]).\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    wrong_corr = 0\n",
    "    caracteres_colles = 0\n",
    "    caracteres_coupes = 0\n",
    "\n",
    "    for image_path, word in dataset:\n",
    "        # Découper l'image en caractères\n",
    "        character_images = split_text_into_characters(image_path)\n",
    "\n",
    "        # Associer les images des caractères au mot correspondant\n",
    "        if len(character_images) == len(word):\n",
    "            processed_data.append((character_images, list(word)))\n",
    "        else:\n",
    "            wrong_corr += 1\n",
    "\n",
    "            if len(character_images) < len(word):\n",
    "                caracteres_colles += 1\n",
    "            \n",
    "            if len(character_images) > len(word):\n",
    "                caracteres_coupes += 1\n",
    "\n",
    "            #print(f\"Attention : le nombre de caractères détectés ne correspond pas au mot '{word}'.\")\n",
    "    \n",
    "    print(f\"Bonnes correspondances = {len(processed_data)}\")\n",
    "    print(f\"Mauvaises correspondances = {wrong_corr}\")\n",
    "    print(f\"Caractères collés = {caracteres_colles}\")\n",
    "    print(f\"Caractères coupés = {caracteres_coupes}\")\n",
    "    return processed_data\n",
    "\n",
    "processed_dataset = process_dataset(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from PIL import Image\n",
    "\n",
    "def split_text_into_characters_tesseract(image_path):\n",
    "    \"\"\"\n",
    "    Découpe une image de mot en images de caractères individuels en utilisant PyTesseract.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): Chemin vers l'image du mot.\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste des images des caractères en format PIL.\n",
    "    \"\"\"\n",
    "    # Charger l'image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convertir en niveaux de gris\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Prétraitement (binarisation)\n",
    "    _, binary_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "    # Utiliser PyTesseract pour détecter les caractères\n",
    "    boxes = pytesseract.image_to_boxes(binary_image, config=\"--psm 7\")\n",
    "\n",
    "    characters = []\n",
    "\n",
    "    # Vérifier si des boîtes ont été détectées\n",
    "    if boxes:\n",
    "        # Chaque ligne de `boxes` correspond à un caractère au format : char x1 y1 x2 y2 page\n",
    "        for box in boxes.splitlines():\n",
    "            box_data = box.split()  # Diviser les informations de la boîte\n",
    "            if len(box_data) == 6:  # Vérifier le format attendu\n",
    "                char, x, y, w, h, _ = box_data\n",
    "                x, y, w, h = map(int, [x, y, w, h])\n",
    "\n",
    "                # Note : PyTesseract donne des coordonnées basées sur l'origine en bas à gauche\n",
    "                y = image.shape[0] - y\n",
    "                h = image.shape[0] - h\n",
    "\n",
    "                # Découper l'image du caractère\n",
    "                char_image = binary_image[h:y, x:w]\n",
    "\n",
    "                # Convertir en format PIL\n",
    "                char_pil = Image.fromarray(char_image)\n",
    "                characters.append(char_pil)\n",
    "\n",
    "    return characters\n",
    "\n",
    "\n",
    "\n",
    "def process_dataset_tesseract(dataset):\n",
    "    \"\"\"\n",
    "    Traite un jeu de données pour découper les images de mots en images de caractères avec PyTesseract.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (list): Liste de tuples (chemin_image, mot).\n",
    "    \n",
    "    Returns:\n",
    "        list: Liste de tuples ([images des caractères], [caractères]).\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    wrong_corr = 0\n",
    "    caracteres_colles = 0\n",
    "    caracteres_coupes = 0\n",
    "\n",
    "    for image_path, word in dataset:\n",
    "        # Découper l'image en caractères\n",
    "        character_images = split_text_into_characters_tesseract(image_path)\n",
    "\n",
    "        # Associer les images des caractères au mot correspondant\n",
    "        if len(character_images) == len(word):\n",
    "            processed_data.append((character_images, list(word)))\n",
    "        else:\n",
    "            wrong_corr += 1\n",
    "\n",
    "            if len(character_images) < len(word):\n",
    "                caracteres_colles += 1\n",
    "            \n",
    "            if len(character_images) > len(word):\n",
    "                caracteres_coupes += 1\n",
    "\n",
    "            #print(f\"Attention : le nombre de caractères détectés ne correspond pas au mot '{word}'.\")\n",
    "    \n",
    "    print(f\"Bonnes correspondances = {len(processed_data)}\")\n",
    "    print(f\"Mauvaises correspondances = {wrong_corr}\")\n",
    "    print(f\"Caractères collés = {caracteres_colles}\")\n",
    "    print(f\"Caractères coupés = {caracteres_coupes}\")\n",
    "    return processed_data\n",
    "\n",
    "processed_dataset = process_dataset_tesseract(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reconnaissance des caractères du mot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(processed_dataset, image_size=(28, 28)):\n",
    "    \"\"\"\n",
    "    Prépare les données pour l'entraînement du modèle neuronal.\n",
    "    \n",
    "    Parameters:\n",
    "        processed_dataset (list): Liste de tuples ([images des caractères], [caractères]).\n",
    "        image_size (tuple): Dimensions des images après redimensionnement (par défaut 28x28).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y) où X est un tableau numpy des images et y les étiquettes correspondantes.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for character_images, word_characters in processed_dataset:\n",
    "        for img, char in zip(character_images, word_characters):\n",
    "            # Redimensionner l'image à la taille spécifiée\n",
    "            img_resized = img.resize(image_size)\n",
    "\n",
    "            # Convertir en tableau numpy\n",
    "            img_array = np.array(img_resized)\n",
    "\n",
    "            # Normaliser les valeurs des pixels (0-255 -> 0-1)\n",
    "            img_normalized = img_array / 255.0\n",
    "\n",
    "            X.append(img_normalized)\n",
    "            y.append(char)\n",
    "\n",
    "    # Convertir en tableaux numpy\n",
    "    X = np.array(X)\n",
    "    X = X.reshape(-1, image_size[0], image_size[1], 1)  # Ajouter la dimension des canaux pour CNN\n",
    "    y = np.array(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "# Préparer les données\n",
    "image_size = (28, 28)  # Taille courante pour les modèles de reconnaissance\n",
    "X, y = prepare_data(processed_dataset, image_size=image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Encoder les étiquettes\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # Convertir les caractères en indices numériques\n",
    "y_one_hot = to_categorical(y_encoded)       # Convertir en one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amandine/newenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_character_recognition_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Crée un modèle CNN pour la reconnaissance de caractères.\n",
    "    \n",
    "    Parameters:\n",
    "        input_shape (tuple): Dimensions de l'image d'entrée (hauteur, largeur, canaux).\n",
    "        num_classes (int): Nombre total de classes (caractères uniques).\n",
    "    \n",
    "    Returns:\n",
    "        model: Modèle Keras.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Couche de convolution\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Deuxième couche de convolution\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "\n",
    "        # Flatten\n",
    "        Flatten(),\n",
    "\n",
    "        # Couche dense\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "\n",
    "        # Couche de sortie\n",
    "        Dense(num_classes, activation='softmax'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Créer le modèle\n",
    "input_shape = (image_size[0], image_size[1], 1)  # Format (28, 28, 1)\n",
    "num_classes = len(label_encoder.classes_)       # Nombre total de caractères uniques\n",
    "model = create_character_recognition_model(input_shape, num_classes)\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.1249 - loss: 3.5025 - val_accuracy: 0.4466 - val_loss: 2.3196\n",
      "Epoch 2/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4033 - loss: 2.4176 - val_accuracy: 0.5058 - val_loss: 1.9462\n",
      "Epoch 3/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.4868 - loss: 2.0579 - val_accuracy: 0.5548 - val_loss: 1.7565\n",
      "Epoch 4/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5122 - loss: 1.8581 - val_accuracy: 0.5680 - val_loss: 1.6950\n",
      "Epoch 5/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5429 - loss: 1.7341 - val_accuracy: 0.5914 - val_loss: 1.6369\n",
      "Epoch 6/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5634 - loss: 1.6669 - val_accuracy: 0.6016 - val_loss: 1.5754\n",
      "Epoch 7/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.5745 - loss: 1.5426 - val_accuracy: 0.6221 - val_loss: 1.5612\n",
      "Epoch 8/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6000 - loss: 1.4617 - val_accuracy: 0.6272 - val_loss: 1.5105\n",
      "Epoch 9/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6280 - loss: 1.3590 - val_accuracy: 0.6323 - val_loss: 1.5246\n",
      "Epoch 10/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6444 - loss: 1.3259 - val_accuracy: 0.6382 - val_loss: 1.5191\n",
      "Epoch 11/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6525 - loss: 1.2468 - val_accuracy: 0.6345 - val_loss: 1.5193\n",
      "Epoch 12/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6527 - loss: 1.2111 - val_accuracy: 0.6433 - val_loss: 1.5099\n",
      "Epoch 13/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6612 - loss: 1.1551 - val_accuracy: 0.6396 - val_loss: 1.5271\n",
      "Epoch 14/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6784 - loss: 1.0871 - val_accuracy: 0.6418 - val_loss: 1.5545\n",
      "Epoch 15/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6891 - loss: 1.0351 - val_accuracy: 0.6440 - val_loss: 1.5864\n",
      "Epoch 16/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6935 - loss: 1.0113 - val_accuracy: 0.6469 - val_loss: 1.5991\n",
      "Epoch 17/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.6998 - loss: 0.9557 - val_accuracy: 0.6396 - val_loss: 1.6007\n",
      "Epoch 18/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7227 - loss: 0.8973 - val_accuracy: 0.6572 - val_loss: 1.6350\n",
      "Epoch 19/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7182 - loss: 0.8791 - val_accuracy: 0.6572 - val_loss: 1.6441\n",
      "Epoch 20/20\n",
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.7399 - loss: 0.8336 - val_accuracy: 0.6513 - val_loss: 1.7275\n"
     ]
    }
   ],
   "source": [
    "# Diviser les données en ensembles d'entraînement et de validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_one_hot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,                # Nombre d'époques\n",
    "    batch_size=32,            # Taille des lots\n",
    "    verbose=1                 # Afficher les logs d'entraînement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Mot prédit : agreetett\n"
     ]
    }
   ],
   "source": [
    "def predict_word(image_path, model, label_encoder, image_size=(28, 28)):\n",
    "    \"\"\"\n",
    "    Prédit un mot à partir d'une image de mot.\n",
    "    \n",
    "    Parameters:\n",
    "        image_path (str): Chemin vers l'image du mot.\n",
    "        model: Modèle entraîné pour la reconnaissance de caractères.\n",
    "        label_encoder: Encodeur pour convertir les prédictions en caractères.\n",
    "        image_size (tuple): Dimensions des images pour le modèle.\n",
    "    \n",
    "    Returns:\n",
    "        str: Mot prédit.\n",
    "    \"\"\"\n",
    "    # Découper l'image en caractères\n",
    "    character_images = split_text_into_characters_tesseract(image_path)\n",
    "\n",
    "    predicted_word = \"\"\n",
    "    for img in character_images:\n",
    "        # Prétraiter l'image\n",
    "        img_resized = img.resize(image_size)\n",
    "        img_array = np.array(img_resized) / 255.0\n",
    "        img_array = img_array.reshape(1, image_size[0], image_size[1], 1)\n",
    "\n",
    "        # Prédire le caractère\n",
    "        prediction = model.predict(img_array)\n",
    "        char = label_encoder.inverse_transform([np.argmax(prediction)])[0]\n",
    "\n",
    "        predicted_word += char\n",
    "\n",
    "    return predicted_word\n",
    "\n",
    "# Exemple d'utilisation\n",
    "predicted_word = predict_word(\"data/agreement.png\", model, label_encoder)\n",
    "print(f\"Mot prédit : {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Affiche l'accuracy et la perte du modèle au cours de l'entraînement.\n",
    "    \n",
    "    Parameters:\n",
    "        history: Objet History retourné par model.fit().\n",
    "    \"\"\"\n",
    "    # Extraire les données de l'entraînement\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1, len(acc) + 1)\n",
    "\n",
    "    # Afficher l'accuracy\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, acc, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Afficher la perte\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, loss, label='Train Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Afficher les courbes\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder le modèle au format TensorFlow\n",
    "model.save('character_recognition_model')\n",
    "\n",
    "# Sauvegarder le modèle au format Keras (.h5)\n",
    "model.save('character_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Charger un modèle sauvegardé au format TensorFlow\n",
    "model = load_model('character_recognition_model')\n",
    "\n",
    "# Charger un modèle sauvegardé au format Keras (.h5)\n",
    "model = load_model('character_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluer le modèle sur les données de validation\n",
    "val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 11:08:09.718206: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-20 11:08:09.740387: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1737367689.778247   29602 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1737367689.788244   29602 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-20 11:08:09.826845: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/amandine/newenv/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-01-20 11:08:14.626218: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 11:08:15.005670: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 150528000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 47/375\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.3943 - loss: 1.7564"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     34\u001b[0m               loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     35\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Entraîner le modèle\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Évaluer le modèle\u001b[39;00m\n\u001b[1;32m     42\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:371\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    370\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 371\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1698\u001b[0m   )\n",
      "File \u001b[0;32m~/newenv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger les données MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Prétraitement des données\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, 10)  # Conversion en one-hot encoding\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Définir le modèle\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')  # 10 classes pour 10 chiffres\n",
    "])\n",
    "\n",
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Entraîner le modèle\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=128,\n",
    "                    validation_split=0.2)\n",
    "\n",
    "# Évaluer le modèle\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Afficher les courbes d'apprentissage\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Sauvegarder le modèle\n",
    "model.save('ocr_model.h5')\n",
    "print(\"Modèle sauvegardé sous le nom 'ocr_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAUTElEQVR4nO3df6xXdf3A8dflIogEkgmKkKhAErhFgUaA+l1auBSngPzoLvnRgHKuaINSNIFaKhKj0okuFSl+mAomzplSSTUpHSLaVNIcaC5uIMoP+SXI+f7ReAWCcs9HfvN4bPzh557XOe/PZ/B53nPu5x6riqIoAgAiot7BXgAAhw5RACCJAgBJFABIogBAEgUAkigAkEQBgCQKsB+88cYbMX78+Pj73/9+sJcCpYgCB1xVVVWMHz/+YC9jv9m6dWv0798/XnjhhejUqVOdZoYMGRKnnXbaLo8d6a8ThyZROIpdeumlcdxxx8X69es/dJuamppo0KBBrF69+gCu7MO9+OKLccUVV8QZZ5wRxx13XJx44olx3nnnxSOPPFLxPpcvXx5VVVX5p7q6Ok499dS4/PLLY8mSJaX39/3vfz+qq6tj5syZUa/evvsntnDhwhg/fnysWbPmY+3nxhtvjG7dukXz5s3j2GOPjfbt28eoUaNi1apV+2ahHNbqH+wFcPDU1NTEI488Eg899FBceeWVu31948aN8fDDD8dFF10Un/rUp/bZcTdt2hT161f2V+/111+P9evXx+DBg+OUU06JjRs3xpw5c+LSSy+NO++8M0aMGFHxugYNGhRf+9rX4v3334+XX345pk6dGo899lj87W9/i86dO9dpH2vWrIlPfvKTMW/evGjUqFHFa4nY/XVauHBhTJgwIYYMGRLNmjWreL/PPvtsdO7cOQYOHBhNmjSJl19+OX75y1/Go48+GkuWLInGjRt/rHVzmCs4am3cuLFo0qRJ0atXrz1+fdasWUVEFPfdd9+H7mPr1q3Fli1b9tcS62Tbtm3F5z73ueLMM8+saH7ZsmVFRBSTJk3a5fF58+YVEVGMGDHiQ2fffffdio75QYMHDy7atGnzkdtMmjSpiIhi2bJl++SYO3vwwQeLiChmz569z/fN4cXlo6NYo0aNok+fPvGHP/whVq5cudvXZ82aFU2aNIlLL700Iv53meWnP/1p/OxnP4u2bdtGw4YN46WXXor33nsvbrjhhujSpUscf/zx0bhx4zj33HPjySef3G2/e7pWvnTp0njjjTcqeh7V1dXx6U9/+mNfVvmgL3/5yxERsWzZsoiIuPfee6Oqqir+9Kc/xVVXXRUtWrSI1q1b5/aPPfZYnHvuudG4ceNo0qRJXHzxxfHiiy/utt/f/va3cdZZZ8Wxxx4bZ511Vjz00EN7PP7Or9P48eNjzJgxERFx+umn56Wu5cuXR0TEW2+9FUuXLo2NGzdW9Fx3/DxjX7+GHH5cPjrK1dTUxPTp0+P++++Pq6++Oh9/++234/HHH49Bgwbtdhlk2rRpsXnz5hgxYkQ0bNgwTjjhhFi3bl3cddddMWjQoBg+fHisX78+7r777ujVq1c888wze7388tnPfjbOP//8WLBgQZ3WvWHDhti0aVOsXbs25s2bF4899lgMGDCg7NP/SK+99lpExG6Xzq666qpo3rx53HDDDbFhw4aIiPj1r38dgwcPjl69esXEiRNj48aNMXXq1OjZs2c899xz+ab7xBNPRN++faNjx45x0003xerVq2Po0KG7xGVP+vTpE6+88krMnj07pkyZEieeeGJERDRv3jwiIm677baYMGFCPPnkk/F///d/e31uRVHE6tWrY9u2bfHqq6/GNddcE9XV1XWa5Qh3sE9VOLi2bdtWtGzZsvjSl760y+N33HFHERHF448/no/tuMzStGnTYuXKlbvt54OXkd55553ipJNOKoYNG7bL4xFRjBs3brfHzj///Dqve+TIkUVEFBFR1KtXr+jXr1/x9ttv13l+Zzue14QJE4pVq1YVtbW1xYIFC4rPf/7zRUQUc+bMKYqiKKZNm1ZERNGzZ89i27ZtOb9+/fqiWbNmxfDhw3fZb21tbXH88cfv8njnzp2Lli1bFmvWrMnHnnjiiSIidrt89MHX6aMuH40bN66IiOLJJ5+s03NesWJFvn4RUbRu3br4zW9+U6dZjmzOFI5y1dXVMXDgwJgyZUosX748v6OdNWtWnHTSSXHBBRfsNtO3b9/8DnXn/VRXV0dExPbt22PNmjWxffv26Nq1ayxevHiv6yhK/r+eRo0aFf369Yt///vfcf/998f7778f7733Xql9fNC4ceNi3Lhx+d9NmzaNiRMnRp8+fXbZbvjw4flcIyLmz58fa9asiUGDBsVbb72Vj1dXV8cXv/jFvIS2YsWKWLJkSVxzzTVx/PHH53Zf+cpXomPHjnnWUYnx48eX+vjqCSecEPPnz4/NmzfHc889F3Pnzo1333234uNz5BAFoqamJqZMmRKzZs2KsWPHxptvvhl/+ctf4jvf+c4ub347nH766Xvcz/Tp02Py5MmxdOnS2Lp16163/zg6dOgQHTp0iIiIK6+8Mr761a9G79694+mnn46qqqqK9jlixIi44oorol69etGsWbPo1KlTNGzYcLftPvh8Xn311Yj4388gPqhp06YR8d9PTkVEtG/ffrdtzjzzzDrFc19p0KBBXHjhhRERcckll8QFF1wQPXr0iBYtWsQll1xywNbBoUcUiC5dukSHDh1i9uzZMXbs2Jg9e3YURRE1NTV73H5PH7WcMWNGDBkyJC677LIYM2ZMtGjRIqqrq+Omm27Ka/P7U79+/WLkyJHxyiuvxJlnnlnRPtq3b59vlB/lg89/+/btEfHfnyucfPLJu21f6cdvD6Tu3btHy5YtY+bMmaJwlDv0/7ZyQNTU1MQPf/jDeOGFF2LWrFnRvn37OPvss+s8/+CDD8YZZ5wRc+fO3eU79Z0vx+xPmzZtioiItWvXHpDj7axt27YREdGiRYuPjEqbNm0i4n9nFjv7xz/+sdfjVHoGVFebN28+KK8fhxYfSSUiIs8KbrjhhliyZMmHniV8mB2XmXb+2cDTTz8df/3rX+s0X9ePpO7po7Nbt26NX/3qV9GoUaPo2LFjHVe87/Tq1SuaNm0aN9544y6XzXbY8ZvCLVu2jM6dO8f06dN3efOdP39+vPTSS3s9zo5fKtvTx0br+pHUDRs27HGbOXPmxDvvvBNdu3bd6zo4sjlTICL+e528e/fu8fDDD0dElI7CJZdcEnPnzo3LL788Lr744li2bFnccccd0bFjxzr9ALOuH0kdOXJkrFu3Ls4777xo1apV1NbWxsyZM2Pp0qUxefLk+MQnPpHb3nvvvTF06NCYNm1aDBkypNTzKaNp06YxderU+MY3vhFf+MIXYuDAgdG8efN444034tFHH40ePXrEbbfdFhERN910U1x88cXRs2fPGDZsWLz99ttx6623RqdOnfb6OnXp0iUiIq677roYOHBgHHPMMdG7d+9o3LhxnT+S+uqrr8aFF14YAwYMiA4dOkS9evVi0aJFMWPGjDjttNPiu9/97j57XTg8iQKppqYmFi5cGOecc060a9eu1OyQIUOitrY27rzzznj88cejY8eOMWPGjHjggQfq/LsHdTFgwIC4++67Y+rUqbF69epo0qRJdOnSJSZOnJi/ZLfDjjfZli1b7rPjf5ivf/3rccopp8TNN98ckyZNii1btkSrVq3i3HPPjaFDh+Z2F110UTzwwANx/fXXx7XXXhtt27aNadOmxcMPP7zX1+nss8+OH//4x3HHHXfE7373u9i+fXssW7as1G0pWrduHX379o0//vGPMX369Ni6dWu0adMmrr766rjuuuv26e1MODxVFWU/CwiHif79+8fy5cvjmWeeOdhLgcOGMwWOSEVRxIIFC2LGjBkHeylwWHGmAEDy6SMAkigAkEQBgCQKAKQ6f/pof/+KPQD7V10+V+RMAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqf7BXgDsTfPmzUvPnHPOOaVnLrvsstIz559/fumZ559/vvRMRMTEiRNLzyxatKiiY3H0cqYAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYBUVRRFUacNq6r291o4wvXu3buiudtvv730TKtWrUrPbNu2rfTMli1bSs80aNCg9ExERG1tbemZnj17lp7517/+VXqGw0Nd3u6dKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAILkhHhXp1q1b6Zl77rmnomM1a9as9Mxdd91VembevHmlZxYtWlR6ZvTo0aVnIiJuueWW0jN333136Znhw4eXnuHw4IZ4AJQiCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIAqf7BXgCHp1NPPbX0TIMGDSo6VufOnUvPrFy5sqJjHWlmzpx5sJfAYcaZAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkNwllYo89dRTpWdGjhxZ0bGOtDueXnjhhQfsWO3atSs9s2DBgn2/EA4bzhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJCqiqIo6rRhVdX+Xgscdr797W+XnvnFL35R0bGWL19eeqZr166lZ9auXVt6hsNDXd7unSkAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACDVP9gLgEPFZz7zmdIz119/femZ6urq0jMREb///e9Lz7i5HWU5UwAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQHJDPI5I3/zmN0vPjB8/vvRMy5YtS8/ceuutpWciIkaPHl3RHJThTAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEhVRVEUddqwqmp/rwX2qF27dqVnli5dWnqmXr3y3yNV8u/izTffLD0TEfHUU0+Vnpk7d27pmfvvv7/0DIeHurzdO1MAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEByQzwq0qtXr9IzgwcPruhY/fv3Lz1Tyc3tKvH666+Xnlm9evV+WMmenXHGGaVnJk6cWHpm8uTJpWe2bdtWeoaPxw3xAChFFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkhviHWEquRHcVVddVXrmJz/5SemZJk2alJ6pVG1tbemZSm7Yt3jx4tIzB/KGeN/61rdKz9x+++2lZ04++eTSMytXriw9w8fjhngAlCIKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgDJDfGOMLfcckvpmdGjR++Hlew7c+bMKT0zYMCA0jPbt28vPXOoO+WUU0rPvPnmm6Vnfv7zn5ee+d73vld6ho/HDfEAKEUUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBS/YO9APatNm3aHJDjbNiwofTMo48+WtGxhg0bVnrmSLy53aFs06ZNB3sJ7CPOFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOQuqUeYUaNGlZ558MEHS8/8+c9/Lj3zn//8p/QMH8+VV155QI4zd+7cA3Ic9j9nCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASFVFURR12rCqan+vBfgIbdu2LT3z7LPPlp5ZsWJF6ZkuXbqUntm4cWPpGT6eurzdO1MAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAECqf7AXAEebTp06VTR3zz33lJ5p2LBh6ZmxY8eWnnFzuyOHMwUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKSqoiiKOm1YVbW/18JO+vfvX9FcvXrlO3/fffdVdKwjTSV/x7t371565kc/+lHpmYiIbt26lZ55/vnnS89U8pw4PNTl7d6ZAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkhviHaL++c9/VjS3bt260jM9evQoPbNp06bSMwfSoEGDSs/07du39EyfPn1Kz2zdurX0TETEuHHjSs/cfPPNFR2LI5Mb4gFQiigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACC5S+oh6oILLqhobv78+aVnNm/eXHrmtddeKz2zePHi0jMREf369Ss906hRo4qOVdbMmTNLz4wZM6aiY9XW1lY0Bzu4SyoApYgCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEByQ7xD1DHHHFPR3A9+8IPSM9dee23pmQN1w7lK3XPPPaVn5syZU3pm0aJFpWdWrVpVegb2BTfEA6AUUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASG6IB3CUcEM8AEoRBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAVL+uGxZFsT/XAcAhwJkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAOn/AR5j0kwvUV78AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction pour afficher une image aléatoire et sa prédiction\n",
    "def display_random_prediction(model, x_data, y_data):\n",
    "    # Sélectionner une image aléatoire\n",
    "    index = np.random.randint(0, len(x_data))\n",
    "    image = x_data[index]\n",
    "    true_label = np.argmax(y_data[index])  # Étiquette réelle\n",
    "\n",
    "    # Faire une prédiction\n",
    "    prediction = model.predict(image.reshape(1, 28, 28, 1))  # Reshape pour correspondre à l'entrée du modèle\n",
    "    predicted_label = np.argmax(prediction)\n",
    "\n",
    "    # Afficher l'image\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    plt.title(f\"Vrai: {true_label}, Prédit: {predicted_label}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Afficher une prédiction pour une image aléatoire\n",
    "display_random_prediction(model, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Mot 1:\n",
      "  Prédit : [8, 8, 8, 8, 8]\n",
      "  Vérité : [1, 2, 3, 4, 5]\n",
      "Mot 2:\n",
      "  Prédit : [8, 8, 8]\n",
      "  Vérité : [7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fonction pour prédire les caractères d'un mot\n",
    "def predict_word(model, word_images):\n",
    "    \"\"\"\n",
    "    Prend un modèle entraîné et une liste d'images de caractères, \n",
    "    retourne le mot prédit.\n",
    "    \"\"\"\n",
    "    predicted_characters = []\n",
    "    for char_img in word_images:\n",
    "        # Redimensionner l'image pour le modèle\n",
    "        char_img = char_img.reshape(1, 28, 28, 1)  # Ajuster selon les dimensions des caractères\n",
    "        # Prédiction du caractère\n",
    "        prediction = model.predict(char_img)\n",
    "        predicted_char = np.argmax(prediction)  # Classe prédite\n",
    "        predicted_characters.append(predicted_char)\n",
    "    return predicted_characters\n",
    "\n",
    "# Fonction principale pour la détection de mots\n",
    "def detect_words(model, data):\n",
    "    \"\"\"\n",
    "    Applique le modèle sur une liste de tuples (images, étiquettes) pour détecter les mots.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for word_images, true_labels in data:\n",
    "        # Prédire les caractères du mot\n",
    "        predicted_word = predict_word(model, word_images)\n",
    "        results.append((predicted_word, true_labels))  # Stocker la prédiction et la vérité\n",
    "    return results\n",
    "\n",
    "# Exemple d'utilisation\n",
    "# Jeu de données : liste de tuples (images des caractères, étiquettes)\n",
    "# Exemple : chaque mot est composé d'une liste de 28x28 images et d'une liste de caractères\n",
    "example_data = [\n",
    "    ([np.random.rand(28, 28) for _ in range(5)], [1, 2, 3, 4, 5]),  # Exemple 1\n",
    "    ([np.random.rand(28, 28) for _ in range(3)], [7, 8, 9])          # Exemple 2\n",
    "]\n",
    "\n",
    "# Appliquer la détection de mots\n",
    "results = detect_words(model, example_data)\n",
    "\n",
    "# Afficher les résultats\n",
    "for i, (predicted, true_labels) in enumerate(results):\n",
    "    print(f\"Mot {i+1}:\")\n",
    "    print(f\"  Prédit : {predicted}\")\n",
    "    print(f\"  Vérité : {true_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x200 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_random_word_prediction(model, data, char_mapping):\n",
    "    \"\"\"\n",
    "    Affiche un mot aléatoire et sa prédiction.\n",
    "    \n",
    "    Parameters:\n",
    "        - model : le modèle entraîné\n",
    "        - data : liste de tuples (images des caractères, étiquettes réelles)\n",
    "        - char_mapping : dictionnaire pour convertir les indices en caractères lisibles\n",
    "    \"\"\"\n",
    "    # Sélectionner un mot aléatoire\n",
    "    random_index = random.randint(0, len(data) - 1)\n",
    "    word_images, true_labels = data[random_index]\n",
    "\n",
    "    # Prédire chaque caractère\n",
    "    predicted_labels = predict_word(model, word_images)\n",
    "\n",
    "    # Convertir les indices en caractères\n",
    "    true_word = ''.join([char_mapping[label] for label in true_labels])\n",
    "    predicted_word = ''.join([char_mapping[label] for label in predicted_labels])\n",
    "\n",
    "    # Afficher les images des caractères\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    for i, char_img in enumerate(word_images):\n",
    "        plt.subplot(1, len(word_images), i + 1)\n",
    "        plt.imshow(char_img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Vrai mot: {true_word}\\nMot prédit: {predicted_word}\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# Exemple de dictionnaire de mappage (à adapter à votre cas)\n",
    "# Si les étiquettes sont des indices [0-9], on peut mapper à des chiffres\n",
    "char_mapping = {i: str(i) for i in range(10)}  # Par exemple, pour des chiffres\n",
    "\n",
    "# Afficher un mot aléatoire et sa prédiction\n",
    "display_random_word_prediction(model, example_data, char_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Étape 0 : Charger les données\n",
    "file_path = \"data/12.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "tuples = [(l.split('`')[0], l.split('`')[1].strip()) for l in lines]\n",
    "\n",
    "# Vérification de l'existence des fichiers\n",
    "result = [(f\"data/{item[0]}\", item[1], os.path.exists(f\"data/{item[0]}\")) for item in tuples]\n",
    "\n",
    "data = []\n",
    "for file, label, exists in result:\n",
    "    if exists:\n",
    "        data.append((file, label))\n",
    "\n",
    "data = data[0:10000]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class WordDataset(Sequence):\n",
    "    def __init__(self, data, batch_size, input_size=(128, 32)):\n",
    "        \"\"\"\n",
    "        data: liste de tuples (chemin_image, texte)\n",
    "        batch_size: taille du batch\n",
    "        input_size: dimensions de l'image redimensionnée\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_data = self.data[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        images, labels = [], []\n",
    "        for image_path, text in batch_data:\n",
    "            # Charger l'image\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, self.input_size) / 255.0  # Normaliser\n",
    "            images.append(image)\n",
    "\n",
    "            # Générer les annotations (boîtes englobantes)\n",
    "            label = self.generate_annotations(image, text)\n",
    "            labels.append(label)\n",
    "\n",
    "        return np.expand_dims(np.array(images), -1), np.array(labels)\n",
    "\n",
    "    def generate_annotations(self, image, text):\n",
    "        \"\"\"\n",
    "        Génère des annotations sous forme de boîtes englobantes pour chaque caractère.\n",
    "        Exemple : [[x_min, y_min, x_max, y_max], ...]\n",
    "        \"\"\"\n",
    "        height, width = image.shape\n",
    "        char_width = width // len(text)\n",
    "        boxes = []\n",
    "        for i, char in enumerate(text):\n",
    "            x_min = i * char_width\n",
    "            x_max = (i + 1) * char_width\n",
    "            y_min = 0\n",
    "            y_max = height\n",
    "            boxes.append([x_min / width, y_min / height, x_max / width, y_max / height])\n",
    "        return np.array(boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_model(input_size=(128, 32, 1), num_boxes=10):\n",
    "    \"\"\"\n",
    "    input_size: dimensions de l'image d'entrée\n",
    "    num_boxes: nombre maximal de caractères dans une image\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=input_size)\n",
    "\n",
    "    # Feature extraction\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "\n",
    "    # Output layer (4 coordonnées par boîte)\n",
    "    outputs = layers.Dense(num_boxes * 4, activation='sigmoid')(x)\n",
    "    outputs = layers.Reshape((num_boxes, 4))(outputs)\n",
    "\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "model = build_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiler le modèle\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Charger vos données\n",
    "# Exemple : data = [(\"chemin/vers/image1.png\", \"MOT\"), ...]\n",
    "train_data = WordDataset(data=train_data, batch_size=32)\n",
    "val_data = WordDataset(data=val_data, batch_size=32)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(train_data, validation_data=val_data, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, image_path, text):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image_resized = cv2.resize(image, (128, 32)) / 255.0\n",
    "    image_input = np.expand_dims(image_resized, axis=(0, -1))\n",
    "\n",
    "    # Prédictions\n",
    "    predicted_boxes = model.predict(image_input)[0]\n",
    "    \n",
    "    # Afficher l'image et les boîtes prédictes\n",
    "    height, width = image.shape\n",
    "    for box in predicted_boxes:\n",
    "        x_min, y_min, x_max, y_max = box\n",
    "        x_min, x_max = int(x_min * width), int(x_max * width)\n",
    "        y_min, y_max = int(y_min * height), int(y_max * height)\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 1)\n",
    "\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "# Exemple\n",
    "visualize_predictions(model, \"chemin/vers/image.png\", \"MOT\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
